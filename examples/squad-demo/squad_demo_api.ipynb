{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD FAISS Demo - Python API\n",
    "\n",
    "This notebook demonstrates the **complete RAGDiff v2.0 workflow** using the Python API, from data preparation to comparison.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example compares two FAISS-based RAG systems using different embedding models:\n",
    "\n",
    "1. **faiss-small**: `paraphrase-MiniLM-L3-v2` (17MB, 3 layers, fast)\n",
    "2. **faiss-large**: `all-MiniLM-L12-v2` (120MB, 12 layers, more accurate)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**Part 1: Data Preparation**\n",
    "- Download and prepare the SQuAD dataset\n",
    "- Build FAISS indices with different embedding models\n",
    "- Generate query sets\n",
    "\n",
    "**Part 2: RAGDiff API Usage**\n",
    "- Execute queries against providers programmatically\n",
    "- Compare results using LLM evaluation\n",
    "- Analyze and export results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preparation\n",
    "\n",
    "First, we'll set up the demo data. This only needs to be run once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have a uv environment set up:\n",
    "\n",
    "```bash\n",
    "# Create virtual environment (if not already created)\n",
    "uv venv\n",
    "\n",
    "# Activate the environment\n",
    "source .venv/bin/activate  # On macOS/Linux\n",
    "# or\n",
    ".venv\\Scripts\\activate  # On Windows\n",
    "\n",
    "# Install RAGDiff in editable mode\n",
    "uv pip install -e .\n",
    "\n",
    "# Start Jupyter\n",
    "uv run jupyter notebook\n",
    "```\n",
    "\n",
    "Once Jupyter is running, you can proceed with the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages with uv...\n",
      "✓ datasets already installed\n",
      "Installing faiss-cpu...\n",
      "✓ faiss-cpu installed\n",
      "✓ sentence-transformers already installed\n",
      "✓ numpy already installed\n",
      "\n",
      "All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using uv\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"datasets\",  # HuggingFace datasets for SQuAD\n",
    "    \"faiss-cpu\",  # FAISS for vector search\n",
    "    \"sentence-transformers\",  # Embedding models\n",
    "    \"numpy\",  # Numerical operations\n",
    "]\n",
    "\n",
    "print(\"Installing required packages with uv...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([\"uv\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download and Prepare SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nfrom datasets import load_dataset\n\n# Set up paths\ndata_dir = Path(\"examples/squad-demo/data\")\ndata_dir.mkdir(parents=True, exist_ok=True)\n\noutput_file = data_dir / \"documents.jsonl\"\nraw_file = data_dir / \"squad_raw.json\"\n\n# Check if already prepared\nif output_file.exists() and raw_file.exists():\n    print(f\"✓ Data already prepared at {data_dir}/\")\n    with open(output_file) as f:\n        num_docs = sum(1 for _ in f)\n    print(f\"  - {num_docs} documents\")\nelse:\n    print(\"Loading SQuAD v2.0 dataset from HuggingFace...\")\n    dataset = load_dataset(\"squad_v2\", split=\"validation\")\n    print(f\"Loaded {len(dataset)} examples\")\n\n    # Extract unique contexts\n    print(\"Extracting unique context paragraphs...\")\n    contexts_seen = set()\n    documents = []\n\n    for idx, example in enumerate(dataset):\n        context = example[\"context\"]\n        if context in contexts_seen:\n            continue\n        contexts_seen.add(context)\n\n        documents.append(\n            {\n                \"id\": f\"squad_{len(documents)}\",\n                \"text\": context,\n                \"source\": \"SQuAD v2.0\",\n                \"metadata\": {\n                    \"title\": example.get(\"title\", \"Unknown\"),\n                    \"original_index\": idx,\n                },\n            }\n        )\n\n    print(f\"Found {len(documents)} unique context paragraphs\")\n\n    # Write documents\n    print(f\"Writing documents to {output_file}...\")\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        for doc in documents:\n            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n\n    # Save raw dataset for query generation\n    print(f\"Saving raw dataset to {raw_file}...\")\n    raw_data = {\n        \"examples\": [\n            {\n                \"id\": ex[\"id\"],\n                \"question\": ex[\"question\"],\n                \"context\": ex[\"context\"],\n                \"answers\": ex[\"answers\"],\n                \"title\": ex.get(\"title\", \"Unknown\"),\n            }\n            for ex in dataset\n        ]\n    }\n\n    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(raw_data, f, indent=2, ensure_ascii=False)\n\n    print(\"\\n✓ Dataset preparation complete!\")\n    print(f\"  Documents: {len(documents)}\")\n    print(f\"  Q&A pairs: {len(dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build FAISS Indices\n",
    "\n",
    "Build two FAISS indices with different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import faiss\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\ndata_dir = Path(\"examples/squad-demo/data\")\ndocuments_file = data_dir / \"documents.jsonl\"\n\n# Load documents\nprint(\"Loading documents...\")\ndocuments = []\nwith open(documents_file, encoding=\"utf-8\") as f:\n    for line in f:\n        documents.append(json.loads(line.strip()))\nprint(f\"Loaded {len(documents)} documents\")\n\ntexts = [doc[\"text\"] for doc in documents]\n\n# Build small model index (fast but less accurate)\nsmall_index_file = data_dir / \"faiss_small.index\"\nif small_index_file.exists():\n    print(f\"\\n✓ Small model index already exists at {small_index_file}\")\nelse:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Building FAISS index with SMALL model (paraphrase-MiniLM-L3-v2)\")\n    print(\"=\" * 60)\n\n    print(\"Loading embedding model (small/fast)...\")\n    model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n\n    print(\"Generating embeddings...\")\n    embeddings = model.encode(\n        texts, show_progress_bar=True, batch_size=32, convert_to_numpy=True\n    )\n    embeddings = np.array(embeddings, dtype=\"float32\")\n\n    print(\n        f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1]}\"\n    )\n\n    print(\"Building FAISS index with L2 distance...\")\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings)\n\n    print(f\"Saving index to {small_index_file}...\")\n    faiss.write_index(index, str(small_index_file))\n\n    print(f\"✓ Small model index created ({index.ntotal} vectors, {index.d} dims)\")\n    print(\"  - 17MB model, 3 layers, fast but less accurate\")\n\n# Build large model index (slower but more accurate)\nlarge_index_file = data_dir / \"faiss_large.index\"\nif large_index_file.exists():\n    print(f\"\\n✓ Large model index already exists at {large_index_file}\")\nelse:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Building FAISS index with LARGE model (all-MiniLM-L12-v2)\")\n    print(\"=\" * 60)\n\n    print(\"Loading embedding model (larger/better quality)...\")\n    model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n\n    print(\"Generating embeddings...\")\n    embeddings = model.encode(\n        texts, show_progress_bar=True, batch_size=8, convert_to_numpy=True\n    )\n    embeddings = np.array(embeddings, dtype=\"float32\")\n\n    print(\n        f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1]}\"\n    )\n\n    print(\"Building FAISS index with L2 distance...\")\n    index = faiss.IndexFlatL2(embeddings.shape[1])\n    index.add(embeddings)\n\n    print(f\"Saving index to {large_index_file}...\")\n    faiss.write_index(index, str(large_index_file))\n\n    print(f\"✓ Large model index created ({index.ntotal} vectors, {index.d} dims)\")\n    print(\"  - 120MB model, 12 layers, slower but more accurate\")\n\nprint(\"\\n✓ All FAISS indices ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Query Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\n\ndata_dir = Path(\"examples/squad-demo/data\")\nraw_file = data_dir / \"squad_raw.json\"\n\nquery_sets_dir = Path(\"examples/squad-demo/domains/squad/query-sets\")\nquery_sets_dir.mkdir(parents=True, exist_ok=True)\n\ntest_queries_file = query_sets_dir / \"test-queries.txt\"\n\nif test_queries_file.exists():\n    with open(test_queries_file) as f:\n        num_queries = sum(1 for _ in f)\n    print(f\"✓ Query set already exists at {test_queries_file}\")\n    print(f\"  - {num_queries} queries\")\nelse:\n    print(\"Generating query sets...\")\n\n    # Load raw dataset\n    with open(raw_file, encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    examples = data[\"examples\"]\n\n    # Filter answerable questions\n    answerable = [ex for ex in examples if ex[\"answers\"][\"text\"]]\n    print(f\"Found {len(answerable)} answerable questions\")\n\n    # Sample 100 questions\n    random.seed(42)\n    sampled = random.sample(answerable, min(100, len(answerable)))\n\n    # Write test queries\n    with open(test_queries_file, \"w\", encoding=\"utf-8\") as f:\n        for ex in sampled:\n            f.write(ex[\"question\"] + \"\\n\")\n\n    print(f\"✓ Created {test_queries_file} ({len(sampled)} queries)\")\n    print(\"\\nExample questions:\")\n    for i, ex in enumerate(sampled[:3], 1):\n        print(f\"  {i}. {ex['question']}\")\n\nprint(\"\\n✓ Data preparation complete! Ready to use RAGDiff API.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: RAGDiff API Usage\n",
    "\n",
    "Now let's use the RAGDiff Python API to compare our providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import RAGDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datetime import datetime\n\nfrom rich.console import Console\n\n# Rich for pretty printing\nfrom rich.table import Table\n\nfrom ragdiff.comparison import compare_runs\nfrom ragdiff.core.loaders import load_domain, load_provider, load_query_set\n\n# RAGDiff v2.0 API\nfrom ragdiff.execution import execute_run\n\nconsole = Console()\n\n# Configuration\ndomain = \"squad\"  # Directory name in domains/\ndomain_dir = Path(\"examples/squad-demo/domains/squad\")\ndomains_dir = domain_dir.parent\nproviders = [\"faiss-small\", \"faiss-large\"]\nquery_set_name = \"test-queries\"\n\nprint(f\"Domain: {domain}\")\nprint(f\"Providers: {providers}\")\nprint(f\"Query set: {query_set_name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Domain Configuration ===\n",
      "Name: squad-demo\n",
      "Description: Example RAG comparison using SQuAD dataset with FAISS providers\n",
      "\n",
      "Evaluator Model: anthropic/claude-sonnet-4-5\n",
      "\n",
      "=== Provider: faiss-small ===\n",
      "Tool: faiss\n",
      "Config: {'index_path': 'examples/squad-demo/data/faiss_small.index', 'documents_path': 'examples/squad-demo/data/documents.jsonl', 'embedding_service': 'sentence-transformers', 'embedding_model': 'paraphrase-MiniLM-L3-v2', 'dimensions': 384}\n",
      "\n",
      "=== Provider: faiss-large ===\n",
      "Tool: faiss\n",
      "Config: {'index_path': 'examples/squad-demo/data/faiss_large.index', 'documents_path': 'examples/squad-demo/data/documents.jsonl', 'embedding_service': 'sentence-transformers', 'embedding_model': 'all-MiniLM-L12-v2', 'dimensions': 384}\n"
     ]
    }
   ],
   "source": [
    "# Load domain configuration\n",
    "domain_config = load_domain(domain, domains_dir)\n",
    "\n",
    "print(\"=== Domain Configuration ===\")\n",
    "print(f\"Name: {domain_config.name}\")\n",
    "print(f\"Description: {domain_config.description}\")\n",
    "print(f\"\\nEvaluator Model: {domain_config.evaluator.model}\")\n",
    "\n",
    "# Load provider configurations\n",
    "for provider_name in providers:\n",
    "    provider_config = load_provider(domain, provider_name, domains_dir)\n",
    "    print(f\"\\n=== Provider: {provider_name} ===\")\n",
    "    print(f\"Tool: {provider_config.tool}\")\n",
    "    print(f\"Config: {provider_config.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Query Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "queries = load_query_set(domain, query_set_name, domains_dir)\n\nprint(f\"Query Set: {query_set_name}\")\nprint(f\"Total queries: {len(queries.queries)}\")\nprint(\"\\nFirst 5 queries:\")\nfor i, query in enumerate(queries.queries[:5], 1):\n    print(f\"{i}. {query.text}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute Runs\n",
    "\n",
    "Execute queries against both providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_callback(current, total, successes, failures):\n",
    "    \"\"\"Progress indicator\"\"\"\n",
    "    if current % 10 == 0 or current == total:\n",
    "        print(\n",
    "            f\"Progress: {current}/{total} queries ({successes} ok, {failures} failed)\"\n",
    "        )\n",
    "\n",
    "\n",
    "runs = {}\n",
    "\n",
    "for provider_name in providers:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Executing run: {provider_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    run = execute_run(\n",
    "        domain=domain,\n",
    "        provider=provider_name,\n",
    "        query_set=query_set_name,\n",
    "        label=f\"{provider_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "        concurrency=10,\n",
    "        per_query_timeout=30.0,\n",
    "        progress_callback=progress_callback,\n",
    "        domains_dir=domains_dir,\n",
    "    )\n",
    "\n",
    "    runs[provider_name] = run\n",
    "\n",
    "    print(f\"\\n✓ Run completed: {run.label}\")\n",
    "    print(f\"  Status: {run.status.value}\")\n",
    "    print(f\"  Successes: {run.metadata.get('successes', 0)}\")\n",
    "    print(f\"  Duration: {run.metadata.get('duration_seconds', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Runs\n",
    "\n",
    "Use LLM evaluation to compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Comparing runs: {providers[0]} vs {providers[1]}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "comparison = compare_runs(\n",
    "    domain=domain,\n",
    "    run_labels=[run.label for run in runs.values()],\n",
    "    concurrency=10,\n",
    "    domains_dir=domains_dir,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Comparison completed\")\n",
    "print(f\"  Duration: {comparison.metadata.get('duration_seconds', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "table = Table(title=\"Comparison Results\")\n",
    "table.add_column(\"Provider\", style=\"cyan\")\n",
    "table.add_column(\"Wins\", style=\"green\")\n",
    "table.add_column(\"Losses\", style=\"red\")\n",
    "table.add_column(\"Ties\", style=\"yellow\")\n",
    "table.add_column(\"Avg Score\", style=\"blue\")\n",
    "table.add_column(\"Avg Latency\", style=\"magenta\")\n",
    "\n",
    "# Calculate stats\n",
    "stats = {\n",
    "    provider: {\"wins\": 0, \"losses\": 0, \"ties\": 0, \"scores\": [], \"latencies\": []}\n",
    "    for provider in providers\n",
    "}\n",
    "\n",
    "for eval_result in comparison.evaluations:\n",
    "    winner = eval_result.winner\n",
    "    if winner == \"tie\":\n",
    "        for provider in providers:\n",
    "            stats[provider][\"ties\"] += 1\n",
    "    else:\n",
    "        stats[winner][\"wins\"] += 1\n",
    "        for provider in providers:\n",
    "            if provider != winner:\n",
    "                stats[provider][\"losses\"] += 1\n",
    "\n",
    "    for provider in providers:\n",
    "        score = eval_result.scores.get(provider, 0)\n",
    "        stats[provider][\"scores\"].append(score)\n",
    "\n",
    "# Get latencies\n",
    "for provider, run in runs.items():\n",
    "    for result in run.results:\n",
    "        if result.latency_ms:\n",
    "            stats[provider][\"latencies\"].append(result.latency_ms)\n",
    "\n",
    "# Add rows\n",
    "for provider in providers:\n",
    "    avg_score = (\n",
    "        sum(stats[provider][\"scores\"]) / len(stats[provider][\"scores\"])\n",
    "        if stats[provider][\"scores\"]\n",
    "        else 0\n",
    "    )\n",
    "    avg_latency = (\n",
    "        sum(stats[provider][\"latencies\"]) / len(stats[provider][\"latencies\"])\n",
    "        if stats[provider][\"latencies\"]\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    table.add_row(\n",
    "        provider,\n",
    "        str(stats[provider][\"wins\"]),\n",
    "        str(stats[provider][\"losses\"]),\n",
    "        str(stats[provider][\"ties\"]),\n",
    "        f\"{avg_score:.1f}\",\n",
    "        f\"{avg_latency:.1f}ms\",\n",
    "    )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "output_file = Path(\"comparison_results.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(comparison.model_dump(mode=\"json\"), f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Comparison exported to: {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete RAGDiff v2.0 workflow:\n",
    "\n",
    "**Part 1: Data Preparation**\n",
    "- ✓ Downloaded SQuAD dataset from HuggingFace\n",
    "- ✓ Built FAISS indices with different embedding models\n",
    "- ✓ Generated query sets for testing\n",
    "\n",
    "**Part 2: RAGDiff API**\n",
    "- ✓ Executed queries against multiple providers\n",
    "- ✓ Compared results using LLM evaluation\n",
    "- ✓ Analyzed and exported results\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The **large model** (all-MiniLM-L12-v2) typically wins more comparisons but is slower\n",
    "- The **small model** (paraphrase-MiniLM-L3-v2) is much faster but less accurate\n",
    "- This demonstrates the classic **quality vs speed tradeoff** in embedding models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Create custom query sets for your domain\n",
    "- Try different embedding models (e.g., all-mpnet-base-v2)\n",
    "- Adjust concurrency for faster execution\n",
    "- Experiment with different LLM evaluators\n",
    "\n",
    "For more information, see the [RAGDiff documentation](../../CLAUDE.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
