{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD FAISS Demo - Python API\n",
    "\n",
    "This notebook demonstrates how to use the **RAGDiff v2.0 Python API** to compare RAG providers programmatically, instead of using the CLI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example compares two FAISS-based RAG systems using different embedding models:\n",
    "\n",
    "1. **faiss-small**: `paraphrase-MiniLM-L3-v2` (17MB, 3 layers, fast)\n",
    "2. **faiss-large**: `all-MiniLM-L12-v2` (120MB, 12 layers, more accurate)\n",
    "\n",
    "We'll demonstrate:\n",
    "- Executing query sets against providers programmatically\n",
    "- Comparing results using LLM evaluation\n",
    "- Analyzing and exporting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# RAGDiff v2.0 API\n",
    "from ragdiff.execution import execute_run\n",
    "from ragdiff.comparison import compare_runs\n",
    "from ragdiff.core.loaders import load_domain, load_provider_config, load_query_set, load_run\n",
    "from ragdiff.core.storage import load_run, list_runs\n",
    "from ragdiff.core.models_v2 import Run\n",
    "\n",
    "# Rich for pretty printing\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up paths to the domain directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain configuration\n",
    "domain = \"squad-demo\"\n",
    "domain_dir = Path(\"domains/squad\")\n",
    "domains_dir = domain_dir.parent  # examples/squad-demo/domains\n",
    "\n",
    "# Providers to compare\n",
    "providers = [\"faiss-small\", \"faiss-large\"]\n",
    "\n",
    "# Query set to use\n",
    "query_set_name = \"test-queries\"\n",
    "\n",
    "print(f\"Domain: {domain}\")\n",
    "print(f\"Domain directory: {domain_dir}\")\n",
    "print(f\"Providers: {providers}\")\n",
    "print(f\"Query set: {query_set_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Domain Configuration\n",
    "\n",
    "Let's first load and inspect the domain configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load domain configuration\n",
    "domain_config = load_domain(domain, domains_dir)\n",
    "\n",
    "print(\"\\n=== Domain Configuration ===\")\n",
    "print(f\"Name: {domain_config.name}\")\n",
    "print(f\"Description: {domain_config.description}\")\n",
    "print(f\"\\nEvaluator:\")\n",
    "print(f\"  Model: {domain_config.evaluator.model}\")\n",
    "print(f\"  Temperature: {domain_config.evaluator.temperature}\")\n",
    "print(f\"  Prompt template (first 100 chars): {domain_config.evaluator.prompt_template[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Provider Configurations\n",
    "\n",
    "Load and inspect the provider configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load provider configurations\n",
    "for provider_name in providers:\n",
    "    provider_config = load_provider_config(domain, provider_name, domains_dir)\n",
    "    \n",
    "    print(f\"\\n=== Provider: {provider_name} ===\")\n",
    "    print(f\"Tool: {provider_config.tool}\")\n",
    "    print(f\"Description: {provider_config.description}\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    for key, value in provider_config.config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Query Set\n",
    "\n",
    "Load the test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query set\n",
    "queries = load_query_set(domain, query_set_name, domains_dir)\n",
    "\n",
    "print(f\"\\n=== Query Set: {query_set_name} ===\")\n",
    "print(f\"Total queries: {len(queries)}\")\n",
    "print(f\"\\nFirst 5 queries:\")\n",
    "for i, query in enumerate(queries[:5], 1):\n",
    "    print(f\"{i}. {query.text}\")\n",
    "    if query.reference:\n",
    "        print(f\"   Reference: {query.reference[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Runs\n",
    "\n",
    "Execute the query set against both providers. This is the programmatic equivalent of:\n",
    "\n",
    "```bash\n",
    "ragdiff run -d domains/squad -p faiss-small -q test-queries\n",
    "ragdiff run -d domains/squad -p faiss-large -q test-queries\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress callback to show execution status\n",
    "def progress_callback(current, total, successes, failures):\n",
    "    \"\"\"Simple progress indicator\"\"\"\n",
    "    if current % 10 == 0 or current == total:\n",
    "        print(f\"Progress: {current}/{total} queries ({successes} ok, {failures} failed)\")\n",
    "\n",
    "# Execute runs for each provider\n",
    "runs = {}\n",
    "\n",
    "for provider_name in providers:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Executing run: {provider_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Execute the run\n",
    "    run = execute_run(\n",
    "        domain=domain,\n",
    "        provider=provider_name,\n",
    "        query_set=query_set_name,\n",
    "        label=f\"{provider_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "        concurrency=10,  # Run 10 queries in parallel\n",
    "        per_query_timeout=30.0,\n",
    "        progress_callback=progress_callback,\n",
    "        domains_dir=domains_dir,\n",
    "    )\n",
    "    \n",
    "    runs[provider_name] = run\n",
    "    \n",
    "    # Display run summary\n",
    "    print(f\"\\n✓ Run completed: {run.label}\")\n",
    "    print(f\"  Run ID: {run.id}\")\n",
    "    print(f\"  Status: {run.status.value}\")\n",
    "    print(f\"  Total Queries: {run.metadata.get('total_queries', 0)}\")\n",
    "    print(f\"  Successes: {run.metadata.get('successes', 0)}\")\n",
    "    print(f\"  Failures: {run.metadata.get('failures', 0)}\")\n",
    "    print(f\"  Duration: {run.metadata.get('duration_seconds', 0):.2f}s\")\n",
    "    print(f\"  Avg latency: {run.metadata.get('avg_latency_ms', 0):.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Run Results\n",
    "\n",
    "Let's look at some individual query results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results for first query from both providers\n",
    "query_index = 0\n",
    "\n",
    "print(f\"\\n=== Query {query_index + 1} ===\")\n",
    "print(f\"Query: {queries[query_index].text}\\n\")\n",
    "\n",
    "for provider_name, run in runs.items():\n",
    "    result = run.results[query_index]\n",
    "    \n",
    "    print(f\"\\n--- {provider_name} ---\")\n",
    "    print(f\"Status: {result.status}\")\n",
    "    print(f\"Latency: {result.latency_ms:.2f}ms\")\n",
    "    print(f\"Chunks retrieved: {len(result.chunks)}\")\n",
    "    \n",
    "    if result.chunks:\n",
    "        print(f\"\\nTop result (score: {result.chunks[0].score:.4f}):\")\n",
    "        print(f\"  {result.chunks[0].content[:200]}...\")\n",
    "    \n",
    "    if result.error:\n",
    "        print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Runs\n",
    "\n",
    "Now let's compare the two runs using LLM evaluation. This is the programmatic equivalent of:\n",
    "\n",
    "```bash\n",
    "ragdiff compare -d domains/squad -r <run-1> -r <run-2>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress callback for comparison\n",
    "def comparison_progress_callback(current, total):\n",
    "    \"\"\"Simple progress indicator for comparison\"\"\"\n",
    "    if current % 10 == 0 or current == total:\n",
    "        print(f\"Evaluating: {current}/{total} queries\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Comparing runs: {providers[0]} vs {providers[1]}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Compare the runs\n",
    "comparison = compare_runs(\n",
    "    domain=domain,\n",
    "    run_labels=[run.label for run in runs.values()],\n",
    "    concurrency=10,  # Evaluate 10 queries in parallel\n",
    "    progress_callback=comparison_progress_callback,\n",
    "    domains_dir=domains_dir,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Comparison completed: {comparison.id}\")\n",
    "print(f\"  Status: {comparison.metadata.get('status', 'completed')}\")\n",
    "print(f\"  Duration: {comparison.metadata.get('duration_seconds', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Comparison Results\n",
    "\n",
    "Display the comparison results in a nice table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "table = Table(title=\"Comparison Results\")\n",
    "table.add_column(\"Provider\", style=\"cyan\")\n",
    "table.add_column(\"Wins\", style=\"green\")\n",
    "table.add_column(\"Losses\", style=\"red\")\n",
    "table.add_column(\"Ties\", style=\"yellow\")\n",
    "table.add_column(\"Avg Score\", style=\"blue\")\n",
    "table.add_column(\"Avg Latency\", style=\"magenta\")\n",
    "\n",
    "# Count wins/losses/ties for each provider\n",
    "stats = {provider: {\"wins\": 0, \"losses\": 0, \"ties\": 0, \"scores\": [], \"latencies\": []} \n",
    "         for provider in providers}\n",
    "\n",
    "for eval_result in comparison.evaluations:\n",
    "    winner = eval_result.winner\n",
    "    \n",
    "    if winner == \"tie\":\n",
    "        for provider in providers:\n",
    "            stats[provider][\"ties\"] += 1\n",
    "    else:\n",
    "        stats[winner][\"wins\"] += 1\n",
    "        for provider in providers:\n",
    "            if provider != winner:\n",
    "                stats[provider][\"losses\"] += 1\n",
    "    \n",
    "    # Collect scores\n",
    "    for provider in providers:\n",
    "        score = eval_result.scores.get(provider, 0)\n",
    "        stats[provider][\"scores\"].append(score)\n",
    "\n",
    "# Get latencies from runs\n",
    "for provider, run in runs.items():\n",
    "    for result in run.results:\n",
    "        if result.latency_ms:\n",
    "            stats[provider][\"latencies\"].append(result.latency_ms)\n",
    "\n",
    "# Add rows to table\n",
    "for provider in providers:\n",
    "    avg_score = sum(stats[provider][\"scores\"]) / len(stats[provider][\"scores\"]) if stats[provider][\"scores\"] else 0\n",
    "    avg_latency = sum(stats[provider][\"latencies\"]) / len(stats[provider][\"latencies\"]) if stats[provider][\"latencies\"] else 0\n",
    "    \n",
    "    table.add_row(\n",
    "        provider,\n",
    "        str(stats[provider][\"wins\"]),\n",
    "        str(stats[provider][\"losses\"]),\n",
    "        str(stats[provider][\"ties\"]),\n",
    "        f\"{avg_score:.1f}\",\n",
    "        f\"{avg_latency:.1f}ms\",\n",
    "    )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examine Individual Evaluations\n",
    "\n",
    "Look at some specific query evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 3 evaluations\n",
    "print(\"\\n=== Sample Evaluations ===\")\n",
    "\n",
    "for i, eval_result in enumerate(comparison.evaluations[:3], 1):\n",
    "    print(f\"\\n--- Query {i} ---\")\n",
    "    print(f\"Query: {eval_result.query}\")\n",
    "    print(f\"\\nWinner: {eval_result.winner}\")\n",
    "    print(f\"\\nScores:\")\n",
    "    for provider, score in eval_result.scores.items():\n",
    "        print(f\"  {provider}: {score}/100\")\n",
    "    print(f\"\\nReasoning: {eval_result.reasoning}\")\n",
    "    print(f\"{'─'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Comparison Results\n",
    "\n",
    "Export the comparison to JSON for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "output_file = Path(\"comparison_results.json\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(comparison.model_dump(mode=\"json\"), f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Comparison exported to: {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Markdown Report\n",
    "\n",
    "Create a markdown report summarizing the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "report_file = Path(\"comparison_report.md\")\n",
    "\n",
    "with open(report_file, \"w\") as f:\n",
    "    f.write(f\"# RAG Comparison Report\\n\\n\")\n",
    "    f.write(f\"**Domain**: {domain}\\n\\n\")\n",
    "    f.write(f\"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"**Query Set**: {query_set_name} ({len(queries)} queries)\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Providers\\n\\n\")\n",
    "    for provider in providers:\n",
    "        f.write(f\"- **{provider}**: {runs[provider].metadata.get('avg_latency_ms', 0):.2f}ms avg latency\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Results\\n\\n\")\n",
    "    f.write(\"| Provider | Wins | Losses | Ties | Avg Score | Avg Latency |\\n\")\n",
    "    f.write(\"|----------|------|--------|------|-----------|-------------|\\n\")\n",
    "    \n",
    "    for provider in providers:\n",
    "        avg_score = sum(stats[provider][\"scores\"]) / len(stats[provider][\"scores\"]) if stats[provider][\"scores\"] else 0\n",
    "        avg_latency = sum(stats[provider][\"latencies\"]) / len(stats[provider][\"latencies\"]) if stats[provider][\"latencies\"] else 0\n",
    "        \n",
    "        f.write(f\"| {provider} | {stats[provider]['wins']} | {stats[provider]['losses']} | \"\n",
    "                f\"{stats[provider]['ties']} | {avg_score:.1f} | {avg_latency:.1f}ms |\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Analysis\\n\\n\")\n",
    "    \n",
    "    # Determine overall winner\n",
    "    winner = max(providers, key=lambda p: stats[p][\"wins\"])\n",
    "    f.write(f\"**Overall Winner**: {winner}\\n\\n\")\n",
    "    \n",
    "    # Quality vs Speed tradeoff\n",
    "    quality_winner = max(providers, key=lambda p: sum(stats[p][\"scores\"]) / len(stats[p][\"scores\"]) if stats[p][\"scores\"] else 0)\n",
    "    speed_winner = min(providers, key=lambda p: sum(stats[p][\"latencies\"]) / len(stats[p][\"latencies\"]) if stats[p][\"latencies\"] else float('inf'))\n",
    "    \n",
    "    f.write(f\"- **Best Quality**: {quality_winner}\\n\")\n",
    "    f.write(f\"- **Fastest**: {speed_winner}\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Sample Evaluations\\n\\n\")\n",
    "    for i, eval_result in enumerate(comparison.evaluations[:5], 1):\n",
    "        f.write(f\"### Query {i}\\n\\n\")\n",
    "        f.write(f\"**Query**: {eval_result.query}\\n\\n\")\n",
    "        f.write(f\"**Winner**: {eval_result.winner}\\n\\n\")\n",
    "        f.write(f\"**Scores**:\\n\")\n",
    "        for provider, score in eval_result.scores.items():\n",
    "            f.write(f\"- {provider}: {score}/100\\n\")\n",
    "        f.write(f\"\\n**Reasoning**: {eval_result.reasoning}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "print(f\"\\n✓ Report exported to: {report_file}\")\n",
    "print(f\"  File size: {report_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Previous Runs (Optional)\n",
    "\n",
    "You can also load and analyze runs that were previously executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available runs for this domain\n",
    "all_runs = list_runs(domain, domains_dir)\n",
    "\n",
    "print(f\"\\n=== Available Runs ===\")\n",
    "print(f\"Total runs found: {len(all_runs)}\\n\")\n",
    "\n",
    "# Show recent runs\n",
    "for run_info in sorted(all_runs, key=lambda x: x['started_at'], reverse=True)[:5]:\n",
    "    print(f\"Label: {run_info['label']}\")\n",
    "    print(f\"  Provider: {run_info['provider']}\")\n",
    "    print(f\"  Query Set: {run_info['query_set']}\")\n",
    "    print(f\"  Status: {run_info['status']}\")\n",
    "    print(f\"  Started: {run_info['started_at']}\")\n",
    "    print(f\"  ID: {run_info['id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific run by label\n",
    "# run_label = \"faiss-small-20240115-120000\"  # Replace with actual label\n",
    "# loaded_run = load_run(domain, run_label, domains_dir)\n",
    "# print(f\"Loaded run: {loaded_run.label}\")\n",
    "# print(f\"  Status: {loaded_run.status}\")\n",
    "# print(f\"  Total queries: {loaded_run.metadata.get('total_queries', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete RAGDiff v2.0 Python API workflow:\n",
    "\n",
    "1. ✓ **Loading configurations**: Domain, provider configs, and query sets\n",
    "2. ✓ **Executing runs**: Running queries against multiple providers in parallel\n",
    "3. ✓ **Comparing results**: Using LLM evaluation to compare provider performance\n",
    "4. ✓ **Analyzing results**: Creating tables and visualizations\n",
    "5. ✓ **Exporting data**: Saving results to JSON and markdown reports\n",
    "6. ✓ **Loading previous runs**: Accessing historical run data\n",
    "\n",
    "### Key API Functions\n",
    "\n",
    "- `execute_run()`: Execute a query set against a provider\n",
    "- `compare_runs()`: Compare multiple runs using LLM evaluation\n",
    "- `load_domain()`, `load_provider_config()`, `load_query_set()`: Load configurations\n",
    "- `load_run()`, `list_runs()`: Access run history\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Create custom query sets for your domain\n",
    "- Add new providers by creating YAML configs\n",
    "- Adjust concurrency for faster execution\n",
    "- Experiment with different LLM models for evaluation\n",
    "- Build custom analysis and visualization tools\n",
    "\n",
    "For more information, see the [RAGDiff documentation](../../CLAUDE.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
