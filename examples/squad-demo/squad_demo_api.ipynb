{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD FAISS Demo - Python API\n",
    "\n",
    "This notebook demonstrates the **complete RAGDiff v2.0 workflow** using the Python API, from data preparation to comparison.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example compares two FAISS-based RAG systems using different embedding models:\n",
    "\n",
    "1. **faiss-small**: `paraphrase-MiniLM-L3-v2` (17MB, 3 layers, fast)\n",
    "2. **faiss-large**: `all-MiniLM-L12-v2` (120MB, 12 layers, more accurate)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**Part 1: Data Preparation**\n",
    "- Download and prepare the SQuAD dataset\n",
    "- Build FAISS indices with different embedding models\n",
    "- Generate query sets\n",
    "\n",
    "**Part 2: RAGDiff API Usage**\n",
    "- Execute queries against providers programmatically\n",
    "- Compare results using LLM evaluation\n",
    "- Analyze and export results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preparation\n",
    "\n",
    "First, we'll set up the demo data. This only needs to be run once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**IMPORTANT: This notebook must be run from the `examples/squad-demo/` directory.**\n",
    "\n",
    "Before running this notebook, ensure you have a uv environment set up:\n",
    "\n",
    "```bash\n",
    "# Navigate to the squad-demo directory\n",
    "cd examples/squad-demo/\n",
    "\n",
    "# Create virtual environment (if not already created in project root)\n",
    "cd ../.. && uv venv && cd examples/squad-demo/\n",
    "\n",
    "# Activate the environment\n",
    "source ../../.venv/bin/activate  # On macOS/Linux\n",
    "# or\n",
    "..\\..\\..venv\\Scripts\\activate  # On Windows\n",
    "\n",
    "# Install RAGDiff in editable mode (from project root)\n",
    "cd ../.. && uv pip install -e . && cd examples/squad-demo/\n",
    "\n",
    "# Start Jupyter from the squad-demo directory\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Once Jupyter is running, you can proceed with the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages with uv...\n",
      "✓ datasets already installed\n",
      "Installing faiss-cpu...\n",
      "✓ faiss-cpu installed\n",
      "✓ sentence-transformers already installed\n",
      "✓ numpy already installed\n",
      "\n",
      "All dependencies installed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using uv\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"datasets\",  # HuggingFace datasets for SQuAD\n",
    "    \"faiss-cpu\",  # FAISS for vector search\n",
    "    \"sentence-transformers\",  # Embedding models\n",
    "    \"numpy\",  # Numerical operations\n",
    "]\n",
    "\n",
    "print(\"Installing required packages with uv...\")\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([\"uv\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download and Prepare SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data already prepared at data/\n",
      "  - 1204 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set up paths - notebook should be run from examples/squad-demo/\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = data_dir / \"documents.jsonl\"\n",
    "raw_file = data_dir / \"squad_raw.json\"\n",
    "\n",
    "# Check if already prepared\n",
    "if output_file.exists() and raw_file.exists():\n",
    "    print(f\"✓ Data already prepared at {data_dir}/\")\n",
    "    with open(output_file) as f:\n",
    "        num_docs = sum(1 for _ in f)\n",
    "    print(f\"  - {num_docs} documents\")\n",
    "else:\n",
    "    print(\"Loading SQuAD v2.0 dataset from HuggingFace...\")\n",
    "    dataset = load_dataset(\"squad_v2\", split=\"validation\")\n",
    "    print(f\"Loaded {len(dataset)} examples\")\n",
    "\n",
    "    # Extract unique contexts\n",
    "    print(\"Extracting unique context paragraphs...\")\n",
    "    contexts_seen = set()\n",
    "    documents = []\n",
    "\n",
    "    for idx, example in enumerate(dataset):\n",
    "        context = example[\"context\"]\n",
    "        if context in contexts_seen:\n",
    "            continue\n",
    "        contexts_seen.add(context)\n",
    "\n",
    "        documents.append(\n",
    "            {\n",
    "                \"id\": f\"squad_{len(documents)}\",\n",
    "                \"text\": context,\n",
    "                \"source\": \"SQuAD v2.0\",\n",
    "                \"metadata\": {\n",
    "                    \"title\": example.get(\"title\", \"Unknown\"),\n",
    "                    \"original_index\": idx,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"Found {len(documents)} unique context paragraphs\")\n",
    "\n",
    "    # Write documents\n",
    "    print(f\"Writing documents to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Save raw dataset for query generation\n",
    "    print(f\"Saving raw dataset to {raw_file}...\")\n",
    "    raw_data = {\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"id\": ex[\"id\"],\n",
    "                \"question\": ex[\"question\"],\n",
    "                \"context\": ex[\"context\"],\n",
    "                \"answers\": ex[\"answers\"],\n",
    "                \"title\": ex.get(\"title\", \"Unknown\"),\n",
    "            }\n",
    "            for ex in dataset\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(raw_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n✓ Dataset preparation complete!\")\n",
    "    print(f\"  Documents: {len(documents)}\")\n",
    "    print(f\"  Q&A pairs: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build FAISS Indices\n",
    "\n",
    "Build two FAISS indices with different embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 1204 documents\n",
      "\n",
      "✓ Small model index already exists at data/faiss_small.index\n",
      "\n",
      "✓ Large model index already exists at data/faiss_large.index\n",
      "\n",
      "✓ All FAISS indices ready!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use paths relative to examples/squad-demo/\n",
    "data_dir = Path(\"data\")\n",
    "documents_file = data_dir / \"documents.jsonl\"\n",
    "\n",
    "# Load documents\n",
    "print(\"Loading documents...\")\n",
    "documents = []\n",
    "with open(documents_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        documents.append(json.loads(line.strip()))\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "\n",
    "# Build small model index (fast but less accurate)\n",
    "small_index_file = data_dir / \"faiss_small.index\"\n",
    "if small_index_file.exists():\n",
    "    print(f\"\\n✓ Small model index already exists at {small_index_file}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Building FAISS index with SMALL model (paraphrase-MiniLM-L3-v2)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Loading embedding model (small/fast)...\")\n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        texts, show_progress_bar=True, batch_size=32, convert_to_numpy=True\n",
    "    )\n",
    "    embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "    print(\n",
    "        f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1]}\"\n",
    "    )\n",
    "\n",
    "    print(\"Building FAISS index with L2 distance...\")\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(f\"Saving index to {small_index_file}...\")\n",
    "    faiss.write_index(index, str(small_index_file))\n",
    "\n",
    "    print(f\"✓ Small model index created ({index.ntotal} vectors, {index.d} dims)\")\n",
    "    print(\"  - 17MB model, 3 layers, fast but less accurate\")\n",
    "\n",
    "# Build large model index (slower but more accurate)\n",
    "large_index_file = data_dir / \"faiss_large.index\"\n",
    "if large_index_file.exists():\n",
    "    print(f\"\\n✓ Large model index already exists at {large_index_file}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Building FAISS index with LARGE model (all-MiniLM-L12-v2)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Loading embedding model (larger/better quality)...\")\n",
    "    model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        texts, show_progress_bar=True, batch_size=8, convert_to_numpy=True\n",
    "    )\n",
    "    embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "    print(\n",
    "        f\"Generated {len(embeddings)} embeddings with dimension {embeddings.shape[1]}\"\n",
    "    )\n",
    "\n",
    "    print(\"Building FAISS index with L2 distance...\")\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(f\"Saving index to {large_index_file}...\")\n",
    "    faiss.write_index(index, str(large_index_file))\n",
    "\n",
    "    print(f\"✓ Large model index created ({index.ntotal} vectors, {index.d} dims)\")\n",
    "    print(\"  - 120MB model, 12 layers, slower but more accurate\")\n",
    "\n",
    "print(\"\\n✓ All FAISS indices ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Query Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Query set already exists at domains/squad/query-sets/test-queries.txt\n",
      "  - 100 queries\n",
      "\n",
      "✓ Data preparation complete! Ready to use RAGDiff API.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Use paths relative to examples/squad-demo/\n",
    "data_dir = Path(\"data\")\n",
    "raw_file = data_dir / \"squad_raw.json\"\n",
    "\n",
    "query_sets_dir = Path(\"domains/squad/query-sets\")\n",
    "query_sets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_queries_file = query_sets_dir / \"test-queries.txt\"\n",
    "\n",
    "if test_queries_file.exists():\n",
    "    with open(test_queries_file) as f:\n",
    "        num_queries = sum(1 for _ in f)\n",
    "    print(f\"✓ Query set already exists at {test_queries_file}\")\n",
    "    print(f\"  - {num_queries} queries\")\n",
    "else:\n",
    "    print(\"Generating query sets...\")\n",
    "\n",
    "    # Load raw dataset\n",
    "    with open(raw_file, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    examples = data[\"examples\"]\n",
    "\n",
    "    # Filter answerable questions\n",
    "    answerable = [ex for ex in examples if ex[\"answers\"][\"text\"]]\n",
    "    print(f\"Found {len(answerable)} answerable questions\")\n",
    "\n",
    "    # Sample 100 questions\n",
    "    random.seed(42)\n",
    "    sampled = random.sample(answerable, min(100, len(answerable)))\n",
    "\n",
    "    # Write test queries\n",
    "    with open(test_queries_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in sampled:\n",
    "            f.write(ex[\"question\"] + \"\\n\")\n",
    "\n",
    "    print(f\"✓ Created {test_queries_file} ({len(sampled)} queries)\")\n",
    "    print(\"\\nExample questions:\")\n",
    "    for i, ex in enumerate(sampled[:3], 1):\n",
    "        print(f\"  {i}. {ex['question']}\")\n",
    "\n",
    "print(\"\\n✓ Data preparation complete! Ready to use RAGDiff API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: RAGDiff API Usage\n",
    "\n",
    "Now let's use the RAGDiff Python API to compare our providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import RAGDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/mwk/Development/ansari-project/ragdiff/examples/squad-demo\n",
      "\n",
      "Contents of current directory:\n",
      "  - README.md\n",
      "  - comparison-report-final.md\n",
      "  - comparison-report-new.md\n",
      "  - comparison-report.md\n",
      "  - domains\n",
      "  - pyproject.toml\n",
      "  - ragdiff_squad_demo.egg-info\n",
      "  - setup_notebook.sh\n",
      "  - squad_demo_api.ipynb\n",
      "  - uv.lock\n",
      "\n",
      "✓ domains/squad/ found relative to current directory\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic cell - run this first to see where you are\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(\"\\nContents of current directory:\")\n",
    "for item in sorted(os.listdir(\".\")[:10]):  # Show first 10 items\n",
    "    print(f\"  - {item}\")\n",
    "\n",
    "# Check if domains/squad exists relative to current directory\n",
    "if Path(\"domains/squad\").exists():\n",
    "    print(\"\\n✓ domains/squad/ found relative to current directory\")\n",
    "else:\n",
    "    print(\"\\n✗ domains/squad/ NOT found relative to current directory\")\n",
    "    print(\"  Will fix this in the next cell...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/mwk/Development/ansari-project/ragdiff/examples/squad-demo\n",
      "Domain: squad\n",
      "Domains directory: /Users/mwk/Development/ansari-project/ragdiff/examples/squad-demo/domains\n",
      "Providers: ['faiss-small', 'faiss-large']\n",
      "Query set: test-queries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "from ragdiff.comparison import compare_runs\n",
    "from ragdiff.core.loaders import load_domain, load_provider, load_query_set\n",
    "from ragdiff.execution import execute_run\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Fix the working directory - just set it to the correct absolute path\n",
    "correct_dir = Path(\"/Users/mwk/Development/ansari-project/ragdiff/examples/squad-demo\")\n",
    "os.chdir(correct_dir)\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Configuration - paths relative to examples/squad-demo/\n",
    "domain = \"squad\"\n",
    "domains_dir = Path(\"domains\")\n",
    "providers = [\"faiss-small\", \"faiss-large\"]\n",
    "query_set_name = \"test-queries\"\n",
    "\n",
    "print(f\"Domain: {domain}\")\n",
    "print(f\"Domains directory: {domains_dir.absolute()}\")\n",
    "print(f\"Providers: {providers}\")\n",
    "print(f\"Query set: {query_set_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Domain Configuration ===\n",
      "Name: squad\n",
      "Description: Example RAG comparison using SQuAD dataset with FAISS providers\n",
      "\n",
      "Evaluator Model: anthropic/claude-sonnet-4-5\n",
      "\n",
      "=== Provider: faiss-small ===\n",
      "Tool: faiss\n",
      "Config: {'index_path': 'data/faiss_small.index', 'documents_path': 'data/documents.jsonl', 'embedding_service': 'sentence-transformers', 'embedding_model': 'paraphrase-MiniLM-L3-v2', 'dimensions': 384}\n",
      "\n",
      "=== Provider: faiss-large ===\n",
      "Tool: faiss\n",
      "Config: {'index_path': 'data/faiss_large.index', 'documents_path': 'data/documents.jsonl', 'embedding_service': 'sentence-transformers', 'embedding_model': 'all-MiniLM-L12-v2', 'dimensions': 384}\n"
     ]
    }
   ],
   "source": [
    "# Load domain configuration\n",
    "domain_config = load_domain(domain, domains_dir)\n",
    "\n",
    "print(\"=== Domain Configuration ===\")\n",
    "print(f\"Name: {domain_config.name}\")\n",
    "print(f\"Description: {domain_config.description}\")\n",
    "print(f\"\\nEvaluator Model: {domain_config.evaluator.model}\")\n",
    "\n",
    "# Load provider configurations\n",
    "for provider_name in providers:\n",
    "    provider_config = load_provider(domain, provider_name, domains_dir)\n",
    "    print(f\"\\n=== Provider: {provider_name} ===\")\n",
    "    print(f\"Tool: {provider_config.tool}\")\n",
    "    print(f\"Config: {provider_config.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Query Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Set: test-queries\n",
      "Total queries: 100\n",
      "\n",
      "First 5 queries:\n",
      "1. Where did France focus its efforts to rebuild its empire?\n",
      "2. What protestant religions made Northern European counties safe for Huguenot immigration?\n",
      "3. BQP and QMA are examples of complexity classes most commonly associated with what type of Turing machine?\n",
      "4. What was Apple Talk\n",
      "5. What happens when bathocyroe and ocyropsis clap their lobes together?\n"
     ]
    }
   ],
   "source": [
    "queries = load_query_set(domain, query_set_name, domains_dir)\n",
    "\n",
    "print(f\"Query Set: {query_set_name}\")\n",
    "print(f\"Total queries: {len(queries.queries)}\")\n",
    "print(\"\\nFirst 5 queries:\")\n",
    "for i, query in enumerate(queries.queries[:5], 1):\n",
    "    print(f\"{i}. {query.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute Runs\n",
    "\n",
    "Execute queries against both providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Executing run: faiss-small\n",
      "============================================================\n",
      "\n",
      "2025-10-28 11:47:05 - ragdiff.execution.executor - INFO - Starting run: domain=squad, provider=faiss-small, query_set=test-queries, concurrency=10\n",
      "2025-10-28 11:47:05 - ragdiff.execution.executor - INFO - Loaded query set with 100 queries\n",
      "2025-10-28 11:47:10 - ragdiff.providers.faiss - INFO - Initialized sentence-transformers model: paraphrase-MiniLM-L3-v2\n",
      "2025-10-28 11:47:10 - ragdiff.providers.faiss - INFO - Loaded FAISS index with 1204 vectors from data/faiss_small.index\n",
      "2025-10-28 11:47:10 - ragdiff.providers.faiss - INFO - Loaded 1204 documents from data/documents.jsonl\n",
      "2025-10-28 11:47:10 - ragdiff.providers.factory - INFO - Created provider 'faiss-small' using tool 'faiss'\n",
      "2025-10-28 11:47:10 - ragdiff.execution.executor - INFO - Created provider instance: FAISSProvider()\n",
      "2025-10-28 11:47:10 - ragdiff.execution.executor - INFO - Run 06366d8d-939d-4bda-ac15-5d09891bd731 status: RUNNING\n",
      "2025-10-28 11:47:10 - ragdiff.execution.executor - INFO - Executing 100 queries with concurrency=10\n",
      "Progress: 10/100 queries (7 ok, 0 failed)\n",
      "Progress: 20/100 queries (20 ok, 0 failed)\n",
      "Progress: 30/100 queries (30 ok, 0 failed)\n",
      "Progress: 40/100 queries (35 ok, 0 failed)\n",
      "Progress: 50/100 queries (50 ok, 0 failed)\n",
      "Progress: 60/100 queries (60 ok, 0 failed)\n",
      "Progress: 70/100 queries (70 ok, 0 failed)\n",
      "Progress: 80/100 queries (80 ok, 0 failed)\n",
      "Progress: 90/100 queries (88 ok, 0 failed)\n",
      "Progress: 100/100 queries (100 ok, 0 failed)\n",
      "2025-10-28 11:47:11 - ragdiff.execution.executor - INFO - Query execution complete: 100 successes, 0 failures\n",
      "2025-10-28 11:47:11 - ragdiff.execution.executor - INFO - Run 06366d8d-939d-4bda-ac15-5d09891bd731 completed: RunStatus.COMPLETED, 100 successes, 0 failures\n",
      "2025-10-28 11:47:11 - ragdiff.core.storage - INFO - Saved run 06366d8d-939d-4bda-ac15-5d09891bd731 to domains/squad/runs/2025-10-28/06366d8d-939d-4bda-ac15-5d09891bd731.json\n",
      "2025-10-28 11:47:11 - ragdiff.execution.executor - INFO - Saved run to domains/squad/runs/2025-10-28/06366d8d-939d-4bda-ac15-5d09891bd731.json\n",
      "\n",
      "✓ Run completed: faiss-small-20251028-114705\n",
      "  Status: completed\n",
      "  Successes: 100\n",
      "  Duration: 5.39s\n",
      "\n",
      "============================================================\n",
      "Executing run: faiss-large\n",
      "============================================================\n",
      "\n",
      "2025-10-28 11:47:11 - ragdiff.execution.executor - INFO - Starting run: domain=squad, provider=faiss-large, query_set=test-queries, concurrency=10\n",
      "2025-10-28 11:47:11 - ragdiff.execution.executor - INFO - Loaded query set with 100 queries\n",
      "2025-10-28 11:47:12 - ragdiff.providers.faiss - INFO - Initialized sentence-transformers model: all-MiniLM-L12-v2\n",
      "2025-10-28 11:47:12 - ragdiff.providers.faiss - INFO - Loaded FAISS index with 1204 vectors from data/faiss_large.index\n",
      "2025-10-28 11:47:12 - ragdiff.providers.faiss - INFO - Loaded 1204 documents from data/documents.jsonl\n",
      "2025-10-28 11:47:12 - ragdiff.providers.factory - INFO - Created provider 'faiss-large' using tool 'faiss'\n",
      "2025-10-28 11:47:12 - ragdiff.execution.executor - INFO - Created provider instance: FAISSProvider()\n",
      "2025-10-28 11:47:12 - ragdiff.execution.executor - INFO - Run a8cb3d67-b261-46e7-8f6c-2b48e4c3ae14 status: RUNNING\n",
      "2025-10-28 11:47:12 - ragdiff.execution.executor - INFO - Executing 100 queries with concurrency=10\n",
      "Progress: 10/100 queries (9 ok, 0 failed)\n",
      "Progress: 20/100 queries (17 ok, 0 failed)\n",
      "Progress: 30/100 queries (30 ok, 0 failed)\n",
      "Progress: 40/100 queries (36 ok, 0 failed)\n",
      "Progress: 50/100 queries (49 ok, 0 failed)\n",
      "Progress: 60/100 queries (60 ok, 0 failed)\n",
      "Progress: 70/100 queries (69 ok, 0 failed)\n",
      "Progress: 80/100 queries (78 ok, 0 failed)\n",
      "Progress: 90/100 queries (90 ok, 0 failed)\n",
      "Progress: 100/100 queries (99 ok, 0 failed)\n",
      "2025-10-28 11:47:13 - ragdiff.execution.executor - INFO - Query execution complete: 100 successes, 0 failures\n",
      "2025-10-28 11:47:13 - ragdiff.execution.executor - INFO - Run a8cb3d67-b261-46e7-8f6c-2b48e4c3ae14 completed: RunStatus.COMPLETED, 100 successes, 0 failures\n",
      "2025-10-28 11:47:13 - ragdiff.core.storage - INFO - Saved run a8cb3d67-b261-46e7-8f6c-2b48e4c3ae14 to domains/squad/runs/2025-10-28/a8cb3d67-b261-46e7-8f6c-2b48e4c3ae14.json\n",
      "2025-10-28 11:47:13 - ragdiff.execution.executor - INFO - Saved run to domains/squad/runs/2025-10-28/a8cb3d67-b261-46e7-8f6c-2b48e4c3ae14.json\n",
      "\n",
      "✓ Run completed: faiss-large-20251028-114711\n",
      "  Status: completed\n",
      "  Successes: 100\n",
      "  Duration: 2.35s\n"
     ]
    }
   ],
   "source": [
    "def progress_callback(current, total, successes, failures):\n",
    "    \"\"\"Progress indicator\"\"\"\n",
    "    if current % 10 == 0 or current == total:\n",
    "        print(\n",
    "            f\"Progress: {current}/{total} queries ({successes} ok, {failures} failed)\"\n",
    "        )\n",
    "\n",
    "\n",
    "runs = {}\n",
    "\n",
    "for provider_name in providers:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Executing run: {provider_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    run = execute_run(\n",
    "        domain=domain,\n",
    "        provider=provider_name,\n",
    "        query_set=query_set_name,\n",
    "        label=f\"{provider_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "        concurrency=10,\n",
    "        per_query_timeout=30.0,\n",
    "        progress_callback=progress_callback,\n",
    "        domains_dir=domains_dir,\n",
    "    )\n",
    "\n",
    "    runs[provider_name] = run\n",
    "\n",
    "    print(f\"\\n✓ Run completed: {run.label}\")\n",
    "    print(f\"  Status: {run.status.value}\")\n",
    "    print(f\"  Successes: {run.metadata.get('successes', 0)}\")\n",
    "    print(f\"  Duration: {run.metadata.get('duration_seconds', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Runs\n",
    "\n",
    "Use LLM evaluation to compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Comparing runs: faiss-small vs faiss-large\n",
      "============================================================\n",
      "\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - INFO - Starting comparison: domain=squad, run_ids=[UUID('06366d8d-939d-4bda-ac15-5d09891bd731'), UUID('a8cb3d67-b261-46e7-8f6c-2b48e4c3ae14')], model=None\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - INFO - Auto-generated label: comparison-20251028-004\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - INFO - Loaded 2 runs\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - INFO - Using evaluator: model=anthropic/claude-sonnet-4-5\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - WARNING - Unknown model prefix for 'anthropic/claude-sonnet-4-5', skipping API key validation\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - INFO - Evaluating 100 queries across 2 runs (concurrency=10)\n",
      "2025-10-28 11:47:13 - ragdiff.comparison.evaluator - INFO - Executing 100 evaluations with concurrency=10\n",
      "Progress: 10/100 evaluations (6 ok, 0 failed)\n",
      "Progress: 20/100 evaluations (20 ok, 0 failed)\n",
      "Progress: 30/100 evaluations (30 ok, 0 failed)\n",
      "Progress: 40/100 evaluations (41 ok, 0 failed)\n",
      "Progress: 50/100 evaluations (49 ok, 0 failed)\n",
      "Progress: 60/100 evaluations (60 ok, 0 failed)\n",
      "Progress: 70/100 evaluations (74 ok, 0 failed)\n",
      "Progress: 80/100 evaluations (80 ok, 0 failed)\n",
      "Progress: 90/100 evaluations (90 ok, 0 failed)\n",
      "Progress: 100/100 evaluations (100 ok, 0 failed)\n",
      "2025-10-28 11:48:15 - ragdiff.comparison.evaluator - INFO - Evaluation complete: 100 successes, 0 failures\n",
      "2025-10-28 11:48:15 - ragdiff.core.storage - INFO - Saved comparison b35698fc-ea10-43b3-bc90-91f927ee51dc to domains/squad/comparisons/2025-10-28/b35698fc-ea10-43b3-bc90-91f927ee51dc.json\n",
      "2025-10-28 11:48:15 - ragdiff.comparison.evaluator - INFO - Saved comparison to domains/squad/comparisons/2025-10-28/b35698fc-ea10-43b3-bc90-91f927ee51dc.json\n",
      "\n",
      "✓ Comparison completed\n",
      "  Duration: 0.00s\n"
     ]
    }
   ],
   "source": [
    "def comparison_progress(current, total, successes, failures):\n",
    "    \"\"\"Progress indicator for comparison\"\"\"\n",
    "    if current % 10 == 0 or current == total:\n",
    "        print(\n",
    "            f\"Progress: {current}/{total} evaluations ({successes} ok, {failures} failed)\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Comparing runs: {providers[0]} vs {providers[1]}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "comparison = compare_runs(\n",
    "    domain=domain,\n",
    "    run_ids=[run.id for run in runs.values()],\n",
    "    concurrency=10,\n",
    "    progress_callback=comparison_progress,\n",
    "    domains_dir=domains_dir,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Comparison completed\")\n",
    "print(f\"  Duration: {comparison.metadata.get('duration_seconds', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Comparison Results                       </span>\n",
       "┏━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Provider    </span>┃<span style=\"font-weight: bold\"> Wins </span>┃<span style=\"font-weight: bold\"> Losses </span>┃<span style=\"font-weight: bold\"> Ties </span>┃<span style=\"font-weight: bold\"> Avg Score </span>┃<span style=\"font-weight: bold\"> Avg Latency </span>┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> faiss-small </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 9    </span>│<span style=\"color: #800000; text-decoration-color: #800000\"> 24     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 45   </span>│<span style=\"color: #000080; text-decoration-color: #000080\"> 63.7      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 51.9ms      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> faiss-large </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 24   </span>│<span style=\"color: #800000; text-decoration-color: #800000\"> 9      </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 45   </span>│<span style=\"color: #000080; text-decoration-color: #000080\"> 68.7      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 75.8ms      </span>│\n",
       "└─────────────┴──────┴────────┴──────┴───────────┴─────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Comparison Results                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mProvider   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWins\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLosses\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTies\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAvg Score\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAvg Latency\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mfaiss-small\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m9   \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m24    \u001b[0m\u001b[31m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m45  \u001b[0m\u001b[33m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m63.7     \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m51.9ms     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mfaiss-large\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m24  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m9     \u001b[0m\u001b[31m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m45  \u001b[0m\u001b[33m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m68.7     \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m75.8ms     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────┴──────┴────────┴──────┴───────────┴─────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create summary table\n",
    "table = Table(title=\"Comparison Results\")\n",
    "table.add_column(\"Provider\", style=\"cyan\")\n",
    "table.add_column(\"Wins\", style=\"green\")\n",
    "table.add_column(\"Losses\", style=\"red\")\n",
    "table.add_column(\"Ties\", style=\"yellow\")\n",
    "table.add_column(\"Avg Score\", style=\"blue\")\n",
    "table.add_column(\"Avg Latency\", style=\"magenta\")\n",
    "\n",
    "# Calculate stats\n",
    "stats = {\n",
    "    provider: {\"wins\": 0, \"losses\": 0, \"ties\": 0, \"scores\": [], \"latencies\": []}\n",
    "    for provider in providers\n",
    "}\n",
    "\n",
    "for eval_result in comparison.evaluations:\n",
    "    evaluation = eval_result.evaluation\n",
    "    winner = evaluation.get(\"winner\", \"unknown\")\n",
    "\n",
    "    # Winner can be provider name, 'a', 'b', 'tie', or 'unknown'\n",
    "    if winner == \"tie\":\n",
    "        for provider in providers:\n",
    "            stats[provider][\"ties\"] += 1\n",
    "    elif winner == \"a\":\n",
    "        stats[providers[0]][\"wins\"] += 1\n",
    "        stats[providers[1]][\"losses\"] += 1\n",
    "    elif winner == \"b\":\n",
    "        stats[providers[1]][\"wins\"] += 1\n",
    "        stats[providers[0]][\"losses\"] += 1\n",
    "    elif winner in stats:\n",
    "        # Winner is actual provider name\n",
    "        stats[winner][\"wins\"] += 1\n",
    "        for provider in providers:\n",
    "            if provider != winner:\n",
    "                stats[provider][\"losses\"] += 1\n",
    "    # else: skip unknown winners\n",
    "\n",
    "    # Try to get scores - they might be score_a/score_b or score_{provider_name}\n",
    "    for provider in providers:\n",
    "        score = (\n",
    "            evaluation.get(f\"score_{provider}\")\n",
    "            or evaluation.get(\"score_a\" if provider == providers[0] else \"score_b\")\n",
    "            or 0\n",
    "        )\n",
    "        if score:\n",
    "            stats[provider][\"scores\"].append(score)\n",
    "\n",
    "# Get latencies from runs\n",
    "for provider, run in runs.items():\n",
    "    for result in run.results:\n",
    "        if result.duration_ms:\n",
    "            stats[provider][\"latencies\"].append(result.duration_ms)\n",
    "\n",
    "# Add rows\n",
    "for provider in providers:\n",
    "    avg_score = (\n",
    "        sum(stats[provider][\"scores\"]) / len(stats[provider][\"scores\"])\n",
    "        if stats[provider][\"scores\"]\n",
    "        else 0\n",
    "    )\n",
    "    avg_latency = (\n",
    "        sum(stats[provider][\"latencies\"]) / len(stats[provider][\"latencies\"])\n",
    "        if stats[provider][\"latencies\"]\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    table.add_row(\n",
    "        provider,\n",
    "        str(stats[provider][\"wins\"]),\n",
    "        str(stats[provider][\"losses\"]),\n",
    "        str(stats[provider][\"ties\"]),\n",
    "        f\"{avg_score:.1f}\",\n",
    "        f\"{avg_latency:.1f}ms\",\n",
    "    )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Export to JSON\noutput_file = Path(\"comparison_results.json\")\nwith open(output_file, \"w\") as f:\n    json.dump(comparison.model_dump(mode=\"json\"), f, indent=2, default=str)\n\nprint(f\"✓ Comparison exported to: {output_file}\")\nprint(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")\n\n# Export to Markdown - using the same format as CLI\nmd_file = Path(\"comparison_results.md\")\n\n# Build markdown content using the same format as the CLI\nlines = [\n    f\"# Comparison: {comparison.label}\",\n    \"\",\n    f\"**Domain:** {comparison.domain}\",\n    f\"**Comparison ID:** {str(comparison.id)[:8]}...\",\n    f\"**Model:** {comparison.evaluator_config.model}\",\n    f\"**Temperature:** {comparison.evaluator_config.temperature}\",\n    \"\",\n    \"## Summary\",\n    \"\",\n    f\"- Total Evaluations: {comparison.metadata.get('total_evaluations', 0)}\",\n    f\"- Successful: {comparison.metadata.get('successful_evaluations', 0)}\",\n    f\"- Failed: {comparison.metadata.get('failed_evaluations', 0)}\",\n    \"\",\n]\n\n# Add provider statistics summary\nlines.extend([\n    \"## Provider Statistics\",\n    \"\",\n    \"| Provider | Wins | Losses | Ties | Avg Score | Avg Latency |\",\n    \"|----------|------|--------|------|-----------|-------------|\",\n])\n\nfor provider in providers:\n    avg_score = (\n        sum(stats[provider][\"scores\"]) / len(stats[provider][\"scores\"])\n        if stats[provider][\"scores\"]\n        else 0\n    )\n    avg_latency = (\n        sum(stats[provider][\"latencies\"]) / len(stats[provider][\"latencies\"])\n        if stats[provider][\"latencies\"]\n        else 0\n    )\n    \n    lines.append(\n        f\"| {provider} | {stats[provider]['wins']} | {stats[provider]['losses']} | \"\n        f\"{stats[provider]['ties']} | {avg_score:.1f} | {avg_latency:.1f}ms |\"\n    )\n\nlines.extend([\"\", \"## Evaluations\", \"\"])\n\n# Add individual evaluations\nfor i, eval_result in enumerate(comparison.evaluations[:20], 1):  # Show first 20\n    lines.append(f\"### {i}. {eval_result.query}\")\n    lines.append(\"\")\n    \n    if eval_result.reference:\n        lines.append(f\"**Reference:** {eval_result.reference}\")\n        lines.append(\"\")\n    \n    if \"winner\" in eval_result.evaluation:\n        lines.append(\n            f\"**Winner:** {eval_result.evaluation.get('winner', 'unknown')}\"\n        )\n        lines.append(\"\")\n    \n    if \"reasoning\" in eval_result.evaluation:\n        reasoning = eval_result.evaluation.get('reasoning', '')\n        lines.append(f\"**Reasoning:** {reasoning}\")\n        lines.append(\"\")\n    \n    # Add scores if present\n    for provider in providers:\n        score = (\n            eval_result.evaluation.get(f\"score_{provider}\")\n            or eval_result.evaluation.get(\"score_a\" if provider == providers[0] else \"score_b\")\n        )\n        if score:\n            lines.append(f\"**Score {provider}:** {score}\")\n    \n    if \"_metadata\" in eval_result.evaluation:\n        metadata = eval_result.evaluation[\"_metadata\"]\n        lines.append(\n            f\"**Cost:** ${metadata.get('cost', 0):.4f}, \"\n            f\"**Tokens:** {metadata.get('total_tokens', 0)}\"\n        )\n    lines.append(\"\")\n\nif len(comparison.evaluations) > 20:\n    lines.append(f\"*... and {len(comparison.evaluations) - 20} more evaluations*\")\n\n# Write markdown file\nmarkdown = \"\\n\".join(lines)\nwith open(md_file, \"w\") as f:\n    f.write(markdown)\n\nprint(f\"✓ Comparison exported to: {md_file}\")\nprint(f\"  File size: {md_file.stat().st_size / 1024:.1f} KB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "output_file = Path(\"comparison_results.json\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(comparison.model_dump(mode=\"json\"), f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Comparison exported to: {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Export to Markdown using the shared utility\n",
    "from ragdiff.display.formatting_v2 import (\n",
    "    calculate_provider_stats_from_runs,\n",
    "    save_comparison_markdown,\n",
    ")\n",
    "\n",
    "md_file = Path(\"comparison_results.md\")\n",
    "\n",
    "# Calculate provider stats from runs and comparison\n",
    "provider_stats = calculate_provider_stats_from_runs(runs, comparison)\n",
    "\n",
    "# Save markdown using the utility function\n",
    "save_comparison_markdown(\n",
    "    comparison,\n",
    "    md_file,\n",
    "    provider_stats=provider_stats,\n",
    "    max_evaluations=20,  # Show first 20 evaluations in notebook\n",
    ")\n",
    "\n",
    "print(f\"✓ Comparison exported to: {md_file}\")\n",
    "print(f\"  File size: {md_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete RAGDiff v2.0 workflow:\n",
    "\n",
    "**Part 1: Data Preparation**\n",
    "- ✓ Downloaded SQuAD dataset from HuggingFace\n",
    "- ✓ Built FAISS indices with different embedding models\n",
    "- ✓ Generated query sets for testing\n",
    "\n",
    "**Part 2: RAGDiff API**\n",
    "- ✓ Executed queries against multiple providers\n",
    "- ✓ Compared results using LLM evaluation\n",
    "- ✓ Analyzed and exported results\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The **large model** (all-MiniLM-L12-v2) typically wins more comparisons but is slower\n",
    "- The **small model** (paraphrase-MiniLM-L3-v2) is much faster but less accurate\n",
    "- This demonstrates the classic **quality vs speed tradeoff** in embedding models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Create custom query sets for your domain\n",
    "- Try different embedding models (e.g., all-mpnet-base-v2)\n",
    "- Adjust concurrency for faster execution\n",
    "- Experiment with different LLM evaluators\n",
    "\n",
    "For more information, see the [RAGDiff documentation](../../CLAUDE.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
